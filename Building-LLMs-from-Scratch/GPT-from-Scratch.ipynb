{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b577c92-3a55-45f3-9169-1ee4a43f7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7b974c1-762b-4c07-a4db-c68bc1dabd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20398\n",
      "﻿I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no\n"
     ]
    }
   ],
   "source": [
    "#Reading in a short story as text sample into Python\n",
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "          raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d37235f-e542-460a-982a-014c0eb36870",
   "metadata": {},
   "source": [
    "### Preprocessing Steps (Chapter 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6197a-3450-476d-a75e-50e66984b1e4",
   "metadata": {},
   "source": [
    "#### 1. Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c490f3ea-71ec-43ef-b2f7-2f6844ab5010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd779b8-4919-4f5f-a6cc-115d8a0edff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "#separting punction and spaces from the text\n",
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a995544a-edaf-41ce-96fa-35a08f74e78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "#remvoing white spaces\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669d64dd-2251-47f6-9c64-7acda8a2b33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text:  ['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "#extending the tokenizer to handle wider range of punctuations\n",
    "text = \"Hello, world. Is this-- a test.\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(\"Tokenized text: \",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52690889-1543-4732-bd56-f97d409d91a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the complete tokenized text from Edith Wharton story: 4690\n"
     ]
    }
   ],
   "source": [
    "#applying this tokenizer to the full Edith Warton's story\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(f\"length of the complete tokenized text from Edith Wharton story: {len(preprocessed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e98ed10-3aa6-4ce4-b66f-3c7c03abba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 tokens: ['\\ufeffI', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "#printing first 30 tokens\n",
    "print(f\"First 30 tokens: {preprocessed[:30]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2729af4-db08-42cc-96da-34851831de5e",
   "metadata": {},
   "source": [
    "#### 2. Converting token into token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8fbec7-c786-45a4-be1d-96ee2f7500b3",
   "metadata": {},
   "source": [
    "create a list of all unique tokens and sort them alphabetically to determine the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4664186-5ff4-426e-940c-d5feb6aba4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1131\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f725dd2-aa85-43c6-8110-32485f318f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n",
      "('His', 51)\n"
     ]
    }
   ],
   "source": [
    "#creating a vocabulary\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9ac1c72-2497-4e7c-bad8-f6e74e43a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing a tokenizer class to tokenize text into tokens, encode tokens to integers and decode integers to tokens\n",
    "\n",
    "class SimpleTokenzierV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab #A\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} #B\n",
    "\n",
    "    def encode(self, text): #C\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids): #D\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) #E\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7efebd56-ea72-4f69-99b4-d070fea10075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating tokenizer object to test the SimpleTokenzierV1 class\n",
    "tokenizer = SimpleTokenzierV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acfbe03a-2dce-45e2-80ed-8c2c97a86623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids for sample text from Edith Warton's story: [1130, 44, 149, 1003, 57, 38, 818, 115, 256, 486, 6, 1002, 115, 500, 435, 392, 6, 908, 585, 1077, 709]\n"
     ]
    }
   ],
   "source": [
    "text = raw_text[:99]\n",
    "ids = tokenizer.encode(text)\n",
    "print(f\"Token ids for sample text from Edith Warton's story: {ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0669898e-04dc-43c2-ad50-4fd8ab344a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text from decoding token ids using SimpleTokenzierV1 class: \n",
      "﻿I HAD always thought Jack Gisburn rather a cheap genius -- though a good fellow enough -- so it was no\n"
     ]
    }
   ],
   "source": [
    "#testing decoder on the token ids above\n",
    "print(f\"text from decoding token ids using SimpleTokenzierV1 class: \\n{tokenizer.decode(ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ee95db7-3368-4bb5-b76e-665cf1c192b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#running the tokenizer on a sample text which is not a part of the text used to create the vocab\u001b[39;00m\n\u001b[1;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mencode(text)\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36mSimpleTokenzierV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      9\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m     10\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m---> 11\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "#running the tokenizer on a sample text which is not a part of the text used to create the vocab\n",
    "text = \"Hello, do you like tea?\"\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c9439-8d76-416b-903d-f72b8bf27588",
   "metadata": {},
   "source": [
    "here, the key error suggests that Hello is not a part of the vocabulary and hence we need to use large and diverse text in order to extend the vocabulary when creating a LARGE language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b41fc-4ba2-4fb0-99ce-8eec285b0daf",
   "metadata": {},
   "source": [
    "#### 3. Adding special context tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54ec79-f0bf-42d1-85b4-8c2cdd90a658",
   "metadata": {},
   "source": [
    "modifying SimpleTokenzierV1 to support new tokens for unknown words and document boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05e2ebda-cfa9-4de8-9e53-6542f783e3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the vocabulary after extending the vocab with unknown words and end of text markers: 1133\n"
     ]
    }
   ],
   "source": [
    "'''adding tokens:\n",
    "1. <unk> - to represent unknown or new words that are not part of the vocabulary\n",
    "2. <|endoftext|> -  marker to separate two different text source from each other\n",
    "'''\n",
    "all_tokens =  sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<unk>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "\n",
    "print(f\"length of the vocabulary after extending the vocab with unknown words and end of text markers: {len(vocab.items())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d66b658-0a72-4ceb-9713-60dca1668b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('\\ufeffI', 1130)\n",
      "('<|endoftext|>', 1131)\n",
      "('<unk>', 1132)\n"
     ]
    }
   ],
   "source": [
    "#printing a sample from the end of the extended vocab\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60289004-5a80-4ed0-8d29-606cc79a57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleTokenzierV2 replaces unknown words with the token \"<unk>\"\n",
    "class SimpleTokenzierV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int\n",
    "                       else \"<unk>\" for item in preprocessed] #A\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) #B\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64600496-2aa1-48d9-97b2-728645d548fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated text with endoftext marker: \n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "#concatenating two unrelated texts\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(f\"concatenated text with endoftext marker: \\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d985105e-7f40-48ac-b4dd-1c13af60c566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1132, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984, 722, 988, 1132, 7]\n"
     ]
    }
   ],
   "source": [
    "#testing the SimpleTokenzierV2 on a text that is concatenation of two unrelated texts\n",
    "tokenizer = SimpleTokenzierV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3edcb7-9f9c-4e50-9a5d-44c39fe8e22f",
   "metadata": {},
   "source": [
    "we can see that the list of token IDs contains 1131 for the <|endoftext|> separator token as well as two 1132 tokens, which are used for unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "992f963f-b798-4355-ad60-62f60a9f5934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>, do you like tea? <|endoftext|> In the sunlit terraces of the <unk>.\n"
     ]
    }
   ],
   "source": [
    "#testing detokenizer for SimpleTokenzierV2\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da483c-0e6b-4b32-903b-d464161a2ae3",
   "metadata": {},
   "source": [
    "#### 4. BPE: Byte pair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ce893-8eae-46ef-9641-b9cb6c426d34",
   "metadata": {},
   "source": [
    "unlike the simple tokenizer implemented above, GPT uses byte pair encoding tokenizer which does not replace unknown words with \"unk\" token but breaks down the word into subword units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8e303342-649a-47ea-a0a1-3286b7992c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b7146f5-becf-4357-a47e-af14281b27ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c35099b-4e45-48f5-bca3-45ef9fb86421",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\") #similar to SimpleTokenzierV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "76efb70b-df40-4aef-b4fc-bfb57e93e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 1059, 430]\n"
     ]
    }
   ],
   "source": [
    "#encoding using BPE\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terra\"\n",
    "integers = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43721223-a769-4c74-820f-b398cb0cf29d",
   "metadata": {},
   "source": [
    "here, the tokenzier handles unknown words by breaking them down into subword units or even individual characters provinding them the largest token id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "57408055-6641-451d-bbd0-a4e15e27f5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terra\n"
     ]
    }
   ],
   "source": [
    "#decoding using BPE\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7896b5c0-3058-4203-91fa-e62af54c2c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token id of unknown words broken down into subword units: [33901, 86, 343, 86, 220, 959]\n",
      "text of unknown words from token ids: Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "text = \"Akwirw ier\"\n",
    "integers = tokenizer.encode(text)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(f\"token id of unknown words broken down into subword units: {integers}\")\n",
    "print(f\"text of unknown words from token ids: {strings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded9f1a-f485-4708-89cf-ce099e3a2cd2",
   "metadata": {},
   "source": [
    "BPE is able to handle unknown words by building a vocab by iteratively merging frequent characters into subwords and frequest subwords into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ca912-8320-4311-a680-30a160191f5a",
   "metadata": {},
   "source": [
    "#### 5. Data Sampling with a sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1546ad16-ada4-4bdb-a902-815c8daf2e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of tokenized Edith Warton Story 5066\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the entire Edith Warton story usinf the BPE tokenizer\n",
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(f\"length of tokenized Edith Warton Story {len(enc_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4406b915-85c7-4da9-9903-99c3fd6c3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a sample of first 50 tokens\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9e2a4da6-50ec-4a00-8e4f-d3b7e4f54c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [5527, 27075, 11, 290]\n",
      "y:        [27075, 11, 290, 4920]\n"
     ]
    }
   ],
   "source": [
    "#creating input-target pairs for next word prediction\n",
    "context_size = 4 #A\n",
    "\n",
    "x = enc_sample[:context_size] #contains the input tokens\n",
    "y = enc_sample[1:context_size+1] #contains targets' inputs shiftd by 1\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:        {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7d86a97f-f6a4-45fc-81fd-43550ca819bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5527] ------> 27075\n",
      "[5527, 27075] ------> 11\n",
      "[5527, 27075, 11] ------> 290\n",
      "[5527, 27075, 11, 290] ------> 4920\n"
     ]
    }
   ],
   "source": [
    "#creating the next-word prediction\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"------>\" , desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8758840b-6325-4a81-afad-f2a7ebb9d491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rich ------>  widow\n",
      " rich widow ------> ,\n",
      " rich widow, ------>  and\n",
      " rich widow, and ------>  established\n"
     ]
    }
   ],
   "source": [
    "#creating the next-word prediction - visualizing the decoded text\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"------>\" , tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c18fecf2-5176-4eae-ba1c-52e19eedbb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing a data loader that iterates over the input dataset and returns inputs & targets as pytorch tensors\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt) #A\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride): #B\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self): #C\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx): #D\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "38eb38f7-2ad5-4e70-9b8b-5b57f20075f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the input in batches via PyTorch DataLoader\n",
    "def create_dataloader_v1(txt, batch_size = 4, max_length = 256, stride = 128, shuffle = True, drop_last = True):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\") #A\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #B\n",
    "    dataloader = DataLoader(dataset, batch_size = batch_size, shuffle = shuffle, drop_last = drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a9c30234-7d4c-45ef-8d7c-05214a08ddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[171, 119, 123,  40]]), tensor([[119, 123,  40, 367]])]\n"
     ]
    }
   ],
   "source": [
    "#testing create_dataloader_v1 with batch_size with 1 for an LLM with context size of 4\n",
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 1, max_length = 4, stride = 1, shuffle = False, drop_last = False)\n",
    "data_iter = iter(dataloader) #A\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3fe7b600-c361-456e-bc74-62f33ef08c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[119, 123,  40, 367]]), tensor([[ 123,   40,  367, 2885]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c2915-b0db-427b-bc27-4f876f98a034",
   "metadata": {},
   "source": [
    "- if we compare the first and the second batch - we can observe that batch token IDs in the second batch have shifted by one position as compared to the first batch\n",
    "- stride setting determines the number of positons the tokens shift across batches, thereby emulating a sliding window approach\n",
    "- if the stride is set equal to the input window size it prevents overlap between batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7c8d8b78-fcbc-4fd4-91e9-2c7d762989d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "tensor([[  171,   119,   123,    40],\n",
      "        [  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284]])\n",
      "Targets: \n",
      "tensor([[  119,   123,    40,   367],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  257,  7026, 15632,   438],\n",
      "        [ 2016,   257,   922,  5891],\n",
      "        [ 1576,   438,   568,   340],\n",
      "        [  373,   645,  1049,  5975],\n",
      "        [  284,   502,   284,  3285]])\n"
     ]
    }
   ],
   "source": [
    "#using the dataloader to sample with a batch size>1\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 8, max_length = 4, stride = 4, shuffle = False, drop_last = False)\n",
    "data_iter = iter(dataloader) #A\n",
    "inputs, targets = next(data_iter)\n",
    "print(f\"Inputs: \\n{inputs}\")\n",
    "print(f\"Targets: \\n{targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412c6af-4ef2-47b6-9cec-bfb65703e120",
   "metadata": {},
   "source": [
    "increasing the stride to 4 ensure no word is skipped and avoids any overlap between batches (more overlap increase overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9402b-bd32-4304-b9b9-43368a3fd0f9",
   "metadata": {},
   "source": [
    "#### 6. Creating token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3ac2d-5217-4751-adc4-c1e5a72be99b",
   "metadata": {},
   "source": [
    "Vector representation of token IDs along with token positions (absolute positional embedding) that will serve as an input data format for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "97434fb7-3f5a-4560-8fd2-8f5afd39c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a sample of four input tokens\n",
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7040bd50-37a9-4d11-abff-a631c5c05852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a vocab size of 6 and embedding size of 3\n",
    "vocab_size = 6\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "93ffd25c-ed5b-465e-966c-d174f42c629f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "'''taking the vocab_size and output_dim to instantiate a embedding layer in PyTorch \n",
    "(setting random seed to 123 for reproducibility)'''\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43207c9-324b-4182-9b89-eb4d25ea22df",
   "metadata": {},
   "source": [
    "- the above matrix is of 6x3 with 6 rows one for each token and 3 columns one for each embedding dimension\n",
    "- these random values are further optimized in the LLM training process via backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bf0877f1-4fd5-48a9-8cef-829567993aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#applying the instantiated embedding layer to a token ID to obtain the embedding vector\n",
    "print(embedding_layer(torch.tensor([3]))) #lookup the embedding vector for this token id in the embedding layer weight matrix above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "826c77d3-d5e1-41c6-90a8-d3f61c003db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#applying the instantiated embedding layer to all token ID to obtain the embedding vector\n",
    "print(embedding_layer(input_ids)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdad657-a965-45d3-bd5b-26cf5d523f1b",
   "metadata": {},
   "source": [
    "#### 7. Encoding word positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0b83d92e-0702-4664-bb93-55964e5bb7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 256\n",
    "vocab_size = 50257\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e39a4103-0eff-4781-8da1-830730c2b84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: \n",
      "tensor([[  171,   119,   123,    40],\n",
      "        [  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284]])\n",
      "Inputs shape: \n",
      "torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "#instantiating the data loader\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size = 8, max_length = max_length, stride = max_length, \n",
    "                                  shuffle = False, drop_last = False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(f\"Token IDs: \\n{inputs}\")\n",
    "print(f\"Inputs shape: \\n{inputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793ab400-1365-40f7-9397-ccc6f05442b0",
   "metadata": {},
   "source": [
    "the token ID tensor of shape 8X4 indicates that the data batch consists 8 text samples with 4 tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aae7b30c-4ede-4b6a-9168-82f2d80cec97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "#using the embedding layer to embed these token ids in 256 dimensional vectors\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "220960c5-9adc-404c-bdf9-4fa78e53bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "'''following the GPT model's absolute positional embedding approach - creating another embedding layer \n",
    "    with same dimension as token_embedding_layer'''\n",
    "\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "57ae2947-6761-4d6b-8fc2-c6a0eb46e682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "#adding the pos_embeddings to token_embeddings in each of the 8 batches\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c6267a-35a6-4b4a-9a39-6d0ec2c262b5",
   "metadata": {},
   "source": [
    "these input_embeddings are now ready to be processed by the main LLM Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c6832-8de6-4aa4-95c0-3ca09ac5598a",
   "metadata": {},
   "source": [
    "### Coding attention mechanism (Chapter 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7f0d014a-813f-4f79-be6f-975bd0beba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a sample input sentence\n",
    "import torch\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your (x^1) \n",
    " [0.55, 0.87, 0.66], # journey (x^2)\n",
    " [0.57, 0.85, 0.64], # starts (x^3)\n",
    " [0.22, 0.58, 0.33], # with (x^4)\n",
    " [0.77, 0.25, 0.10], # one (x^5)\n",
    " [0.05, 0.80, 0.55]] # step (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b77fdb-6bc1-4fd0-a932-7825db430518",
   "metadata": {},
   "source": [
    "#### 8. Computing intermediate attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b8c48510-d7b3-46ea-9ad5-371a360ab57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the computed intermediate attention scores are: \n",
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "#computing intermediate attention scores between query and input token by taking dot product of query with input token\n",
    "query = inputs[1] #A\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(f\"the computed intermediate attention scores are: \\n{attn_scores_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b45e8-bf01-4040-b9b0-e2b574281b43",
   "metadata": {},
   "source": [
    "#### 9. Obtaining attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2338eb7f-e26f-4a03-ab85-33f2a7aaeba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "sum of attention weights: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "#obtaining attention weights (that sum up to 1) by normalizing the attention scores\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(f\"attention weights: {attn_weights_2_tmp}\")\n",
    "print(f\"sum of attention weights: {attn_weights_2_tmp.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "21db5a44-bfcf-4b14-ae3b-f325e8f85bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "sum of attention weights: 1.0\n"
     ]
    }
   ],
   "source": [
    "'''softmax function is commonly used to normalize the attention scores \n",
    "(can handle extreme values, ensures attn. weights are alays positive etc.)'''\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim = 0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(f\"attention weights: {attn_weights_2_naive}\")\n",
    "print(f\"sum of attention weights: {attn_weights_2_naive.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a6c9b51e-f747-436b-8651-6c56f7eb7d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "sum of attention weights: 1.0\n"
     ]
    }
   ],
   "source": [
    "#using PyTorch softmax funtion to prevent overflow and underflow while dealing with large or small input values\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim = 0)\n",
    "print(f\"attention weights: {attn_weights_2}\")\n",
    "print(f\"sum of attention weights: {attn_weights_2.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e005f4c-fcf9-4592-bfe3-2a0c5071bd62",
   "metadata": {},
   "source": [
    "#### 10. Calculating context vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "61f35315-de17-4178-8368-8f574d5dec71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "'''calculatin context vector by multiplying embedded input tokens with corresponding attention weights \n",
    "and then summing the resulting vectors'''\n",
    "query = inputs[1] #second input token is the query\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c658dcb2-1622-41b1-a4bd-b16e25885e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#extending the above implementation to calculate the attention weights and context vector for all inputs\n",
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9c30f6e4-fd73-43c7-9ebc-cb116237ae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#getting the attn scores without for loop\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9f81ef3f-2681-4f93-b121-9e87f9785193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "#normalizing each row in the above output to sum up the values to 1\n",
    "attn_weights  = torch.softmax(attn_scores, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "082f89de-17e3-46ed-87f0-5d8846c3820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 0.9999\n",
      "All row sum: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "#verifying the normalization\n",
    "row_2_sum = sum([0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452])\n",
    "print(f\"Row 2 sum: {row_2_sum}\")\n",
    "print(f\"All row sum: {attn_weights.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4431b7bb-78b4-4c83-a61e-af620c25ee29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "#getting context vectors\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fa6d1696-dfbb-4018-a98c-52aacca0d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "#cross validating th context vecs with the previously generated context vec for second row\n",
    "print(f\"Previous 2nd context vector: {context_vec_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ebb682-380b-412b-92d5-601e15815b10",
   "metadata": {},
   "source": [
    "#### 11. Implementing self-attention with trainable weights\n",
    "Enable LLM to learn from data and improve its performance on specific tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9676a9fb-8bb5-426c-8096-fd0bf550702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computing attention weights step by step\n",
    "x_2 = inputs[1] #A\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2 #C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "22263d25-f21f-4fa6-b471-0a37d060a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intializing 3 weight metrics\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad =  False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad =  False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3f79a0fc-7143-4ea6-9d52-3ba027565ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "#computing query, key, and value vectors\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6b5e288c-b3da-4cb7-b060-73114686cf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "value.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "#obtaining all keys and values\n",
    "keys = inputs @ W_key\n",
    "value = inputs @ W_value\n",
    "print(f\"keys.shape: {keys.shape}\")\n",
    "print(f\"value.shape: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b73e3c48-2186-4d81-87b8-14b21bb611d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "#computing attention scores\n",
    "keys_2 = keys[1] #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7f227445-e41f-41ea-a2e0-495288e83d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "#applying across all attention scores\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b196aeb5-c3d3-4277-8705-37c4da1bfd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "'''normalizing the attention scores using softmax to get attention weights.\n",
    "scaling attention scores by dividing them by square root (or exponentiating by 0.5) of the embedding dimension of the keys\n",
    "'''\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k ** 0.5, dim = -1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f5398d99-08a4-4a8f-bf5d-2789d1a45237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "#computing context vectors\n",
    "context_vec_2 = attn_weights_2 @ value\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8ce1a212-1793-4234-902e-82af76913861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing a compact self-attention python class\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        '''initializes trainable weights for:\n",
    "        queries, keys, values -- each transforming the input \n",
    "        with dimension d_in to dimension d_out\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''computing attention scores'''\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attn_scores = queries @ keys.T #omega\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e2fb24b4-af12-420b-907b-7db90d414d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#using the above class\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ecec13-9bf6-4865-a864-13337a4cf0c0",
   "metadata": {},
   "source": [
    "the second row here matches with the context_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7e801ad8-efda-4ab4-8c81-ae094707f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#self-attention class using PyTorch's Linear layers (has optimized weight intialization scheme)\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "71b9f0da-3e4d-4597-bb48-e9912d3e86a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#using the above class\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35293227-3ff5-434f-a64d-056399f43209",
   "metadata": {},
   "source": [
    "both sa_v1 and sa_v2 have different outputs because they use different initial weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf5a8f-eab9-4fb3-a960-84ee558b5711",
   "metadata": {},
   "source": [
    "#### 12. Hiding future words with casual attention (masked attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c7c3bbed-b612-40aa-99e0-13f7688f0cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#computing attention weights using softmax function\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = -1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e1268b19-ffe0-4612-bb14-d5920542f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#using PyTorch tril function to create a mask where values above the diagnol are zero\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d07c3736-a496-46d0-80eb-33ee182425a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#multiplying the mask with attention weights to zero out the values above the diagnol\n",
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "11ad9f6e-c0c4-4056-9b48-79a394dd36a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#renormalizing the attention weights to sum up the weight to 1 in each row\n",
    "row_sums = masked_simple.sum(dim = 1, keepdim = True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4ef344dd-c6ac-4912-bb13-b5feb96f306f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#masking attention scores with negative infinity values\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal = 1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "96b5631a-8c8e-4db3-a5d0-3a0639d193de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#applying softmax function on the masked results to normalize each row\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim = 1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c6e6f-04d9-4100-9936-503ab666ddf1",
   "metadata": {},
   "source": [
    "Softmax function converts intputs to probability distribution and treats negative infinity values as zero probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36abad9-9d15-4b0e-829a-9918d46b2907",
   "metadata": {},
   "source": [
    "#### 13. Masking additonal attention weights with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e28ac1-9bbd-481c-a5a8-924c505b0aee",
   "metadata": {},
   "source": [
    "DL technique where randomly selected layers are ignored during training (dropping them out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "27f0373a-4606-4a46-80e6-94ac1096efc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#implementing drop out rate of 50% - masking 50% of attention weights\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18558e56-ac67-4911-b52a-e4a3bd4ef25f",
   "metadata": {},
   "source": [
    "- random elements are set to zero and the remaining values as scaled by a factor of 1/0.5 to compensate for this reduction in elements.\n",
    "- this scaling helps maintain the average influence of the attention mechanism during both training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7364a6c4-d7e6-4f1b-a81f-8421f75ca8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#applying drop out to attention weight matrix\n",
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4fba2a49-5d49-4bdf-9ba8-f1d8058047ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing compact casual attention class\n",
    "class CasualAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) #A\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal = 1)) #B\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape #batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) #C\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) #D\n",
    "        attn_weights = torch.softmax(attn_scores / (keys.shape[-1]**0.5), dim = -1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5fb2fe15-e5ca-4107-a0ff-1e155f181e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "#using the CasualAttention class\n",
    "batch_size = 2\n",
    "batch = torch.randn(batch_size, context_length, d_in)  \n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CasualAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(f\"context_vecs shape: {context_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad2196-dc5e-4f34-ac09-a684cf1577d9",
   "metadata": {},
   "source": [
    "#### 14. Extending single-head attention to multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c783a0-96ac-49e3-ae35-f29bd9e51f92",
   "metadata": {},
   "source": [
    "dividing attention mechanism into mutiple head - each operating independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "40b9a545-decc-4127-822d-fafb952657f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking multiple single-head attention layers\n",
    "\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CasualAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "                                   for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3a026232-f840-4d11-b25d-5dfecab2058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4593, -0.2785, -0.4767, -0.1330],\n",
      "         [ 0.4732, -0.1399, -0.4944, -0.1421],\n",
      "         [ 0.1026, -0.4158, -0.1594,  0.2188],\n",
      "         [ 0.1373, -0.1506, -0.1594,  0.0565],\n",
      "         [-0.3830, -0.0754,  0.1471,  0.2819],\n",
      "         [-0.0794, -0.2116,  0.1015,  0.3038]],\n",
      "\n",
      "        [[ 0.1174,  1.1445,  0.0553, -0.7877],\n",
      "         [ 0.3912,  0.9934, -0.3112, -0.7816],\n",
      "         [ 0.3936,  0.9043, -0.3300, -0.6898],\n",
      "         [ 0.2705,  0.4034, -0.2218, -0.4146],\n",
      "         [ 0.3510,  0.6279, -0.2101, -0.1913],\n",
      "         [ 0.3235,  0.6054, -0.2006, -0.2066]]], grad_fn=<CatBackward0>)\n",
      "context_vecs shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "#using MultiHeadAttentionWrapper class\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] #number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads = 2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(f\"context_vecs shape: {context_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "346242f7-31aa-4541-902c-844c63c72596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing multi-head attention with weight splits\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0 #d_out must be visible by num\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads #A\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) #B\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal = 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x) #C\n",
    "        queries = self.W_query(x) #C\n",
    "        values = self.W_value(x) #C\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim) #D\n",
    "\n",
    "        keys = keys.transpose(1, 2) #E\n",
    "        queries = queries.transpose(1, 2) #E\n",
    "        values = values.transpose(1, 2) #E\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3) #F\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens] #G\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) #H\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) #I #J\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) #K\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a7ea5b9d-de21-431b-a7c1-dd37baba3aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0536, 0.8755],\n",
      "         [0.0841, 0.9012],\n",
      "         [0.0730, 0.6893],\n",
      "         [0.1485, 0.7239],\n",
      "         [0.2545, 0.4834],\n",
      "         [0.1575, 0.5968]],\n",
      "\n",
      "        [[0.4336, 0.8920],\n",
      "         [0.3494, 1.0295],\n",
      "         [0.3310, 1.0099],\n",
      "         [0.2171, 0.8726],\n",
      "         [0.2820, 0.9429],\n",
      "         [0.2826, 0.9248]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "#using the MultiHeadAttention class\n",
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads = 2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(f\"context_vecs shape: {context_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11885e4e-631d-431d-adaf-1c1fcc5fa116",
   "metadata": {},
   "source": [
    "### Implementing a GPT model from scratch to generate text (Chapter 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f52f84b-ae0b-4bc1-be25-94cebc7db9a4",
   "metadata": {},
   "source": [
    "#### 15. Implementing a dummy GPT model architecture class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8d83f428-66d5-4a68-a6bb-e810dfa71961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conifguration of small GPT 2 model\n",
    "GPT_CONFIG_124M = {\n",
    "   \"vocab_size\": 50257,  # Vocabulary size\n",
    "   \"context_length\": 1024,  # Context length\n",
    "   \"emb_dim\": 768,  # Embedding dimension\n",
    "   \"n_heads\": 12,  # Number of attention heads\n",
    "   \"n_layers\": 12,  # Number of layers\n",
    "   \"drop_rate\": 0.1,  # Dropout rate\n",
    "   \"qkv_bias\": False  # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bd110fe0-f080-42a3-8949-bbf9b56fdf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "30475e6b-d1ff-44cf-a4b5-a6c7b6f73b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias = False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3b355b-1a0f-4637-8b09-14e0a6f13103",
   "metadata": {},
   "source": [
    "- DummyGPTModel is a simplified version of GPT like model using pytorch neural network module.\n",
    "- It has token and positional embeddings, dropout, a series of tranformer blocks(DummyTransformerBlock), a final layer of normalization(DummyLayerNorm) and a linear output layer (out_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "98750459-3073-42fa-b1e0-69ccda7e0e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "#preparing input data and initializing GPT model\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a6a7853f-dec9-4c68-b8aa-6dc6c46a0754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#initializing 124 million parameter DummyGPTModel instance and feed it the tokenized batch\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05061467-9186-42b8-ac40-d36ec69eb6ba",
   "metadata": {},
   "source": [
    "- output tensor has two rows for 2 text samples with 4 tokens each being a 50257 dimensional vector - matches the size of the tokenizer vocabulary\n",
    "- Each of the 50267 dimensions refers to a unique token in the vocabulary which is later converted into token IDs to decode into words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d628b-cd7a-4719-a3c6-0288de8b9459",
   "metadata": {},
   "source": [
    "#### 16. Normalizing activations with layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d6f9cf-1bca-4323-a307-7edaa5a0d350",
   "metadata": {},
   "source": [
    "- training deep neural networks is challenging due to vanishing and exploding gradients as it prevents the learning process from finding the right set of parameters that reduce the loss function and hence we implement layer normalization to adjust the activations of the NN to have a unit variance (mean = 0, variance = 1)\n",
    "- this speeds up convergence to effecitve weights and ensures proper training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "6f6a4338-2c06-4a1e-89bf-d13745f59677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#implementing neural network layers with 5 inputs and 6 output - applying to tow inputs\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5) #A\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ba23c-d46c-4822-a3f8-86345c72e0ed",
   "metadata": {},
   "source": [
    "two rows with layer outputs from each input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "041e199c-8e99-4195-89b5-41ac0360f2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#examining mean and variance for each input row\n",
    "mean = out.mean(dim = -1, keepdim = True)\n",
    "var = out.var(dim = -1, keepdim = True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "312ded50-8320-4503-bb6b-e53a67379c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n",
      "\n",
      "\n",
      "printing outputs for better visbility by turning off scientific notation:\n",
      "\n",
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#applying layer normalization to layer outputs\n",
    "out_norm = (out - mean)/torch.sqrt(var)\n",
    "mean = out_norm.mean(dim = -1, keepdim = True)\n",
    "var = out_norm.var(dim = -1, keepdim = True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "print(\"\\n\\nprinting outputs for better visbility by turning off scientific notation:\\n\")\n",
    "torch.set_printoptions(sci_mode = False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab74b8-7e0c-43ac-aa72-e72710b22ddc",
   "metadata": {},
   "source": [
    "the mean and variance of normalized layers are 0 & 1 respectively post apllying normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "9df4a76a-d440-45e4-b93d-d0afe047c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer normalization class\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True, unbiased = False)\n",
    "        norm_x = (x - mean)/torch.sqrt(var + self.eps) #eps prevents division by 0 during normalization\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "c15a43a0-77e4-44d9-b3e5-eb1d10c1e6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#applying layer norm to batch input\n",
    "ln = LayerNorm(emb_dim = 5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim = -1, keepdim = True)\n",
    "var = out_ln.var(dim = -1, unbiased = False, keepdim = True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed7a7a8-38db-4d3d-b8dc-3b031b80de6e",
   "metadata": {},
   "source": [
    "#### 17. Implementing feed forward neural networks with GELU activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "5c6166aa-6b2c-4800-953c-81c3ee47c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi))* (x + 0.044715 * torch.pow(x, 3)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "b396950a-6f22-4104-aad0-7239eb83669e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "gelu, relu = nn.GELU(), nn.ReLU()\n",
    "x = torch.linspace(-3, 3, 100)  # A\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"])):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efca295-cfd6-40c3-9e9d-b3e9db5824f5",
   "metadata": {},
   "source": [
    "GELU has no-zero gradient for negative values as compared to ReLU (which is 0 for negative values which sometimes make optimizations harder has is doen't allow nuanced outputs). GELU on the other hand allows even the neurons that have recioeved negative input to contribute to learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "713b2221-cfbb-4361-95c9-5dc811fb34dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed forward class - consisting of two linear layers\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), #Linear transformation --> increases embedding dimension by a factor of 4\n",
    "            GELU(), #non-linear GELU activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]) #Linear transformation --> decreases embedding dimension by a factor of 4\n",
    "                                    ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "b515925f-fa1a-4f53-88a3-59a4ac503bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "#initializing FeedForward class with batch size of 2, token 3 and embedding size 768\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768) #A\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0bad95-37bf-48ad-bc10-497cb004e235",
   "metadata": {},
   "source": [
    "#### 18. Adding shortcut connections (skip connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "763d212c-04f3-4beb-94c2-baa2551da959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#illustrating shortcut connections - with 5 NN layers\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "            nn.Linear(layer_sizes[0], layer_sizes[1]), \n",
    "            GELU()),\n",
    "            nn.Sequential(\n",
    "            nn.Linear(layer_sizes[1], layer_sizes[2]), \n",
    "            GELU()),\n",
    "            nn.Sequential(\n",
    "            nn.Linear(layer_sizes[2], layer_sizes[3]), \n",
    "            GELU()),\n",
    "            nn.Sequential(\n",
    "            nn.Linear(layer_sizes[3], layer_sizes[4]), \n",
    "            GELU()),\n",
    "            nn.Sequential(\n",
    "            nn.Linear(layer_sizes[4], layer_sizes[5]), \n",
    "            GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            #compute the output of the current layer\n",
    "            layer_output - layer(x)\n",
    "            #check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "\n",
    "            else:\n",
    "                x = layer_output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "c40465eb-1c51-4bdc-a836-f990b67be65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing neural network without shortcut connections\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) #specifying random seed for initial weight\n",
    "model_wihtout_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
